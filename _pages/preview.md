---
permalink: /preview
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  # - /preview/
  # - /preview.html
---

<button onclick="topFunction()" id="toTopBtn" title="toTop">to TOP</button>

<span class='anchor' id='index'></span>

# ğŸ“„ About Me
I am currently an Associate Professor at the School of Artificial Intelligence, <a href='https://en.wikipedia.org/wiki/Beijing_Normal_University' target="_blank">Beijing Normal University</a>.

My research interests focus on human-centered computer vision and graphics, including motion capture, reconstruction, rendering, and the synthesis of digital humans. I work closely with <a href="https://liuyebin.com" target="_blank">Prof. Yebin Liu</a>.

I am looking for self-motivated Ph.D./Master students with strong research interests in 3D Vision, Computer Graphics, and Embodied AI.

<a href='https://ai.bnu.edu.cn/xygk/szdw/fgj/9b9ed1de20d3445fbdd4a57b72b2b885.htm' target="_blank">[ä¸­æ–‡ç®€ä»‹]</a><br />
æ¬¢è¿åŒ—å¸ˆå¤§æ ¡å†…å¤–åŒå­¦è”ç³»å®ä¹ ã€ç”³è¯·ç¡•å£«/åšå£«ç ”ç©¶ç”Ÿ [<a shape="rect" href="javascript:togglebib('recruitment')" class="togglebib">æ‹›ç”Ÿè¯´æ˜</a>]<br />
<!-- <strong style="color:red;">2025å¹´å…¥å­¦æœ¬äººå‰©ä½™åé¢ï¼š1ååšå£«ï¼›è¯¾é¢˜ç»„åˆä½œæŒ‡å¯¼å‰©ä½™åé¢ï¼š1åç¡•å£«ï¼›</strong><br /> -->
<strong style="color:red;">æœ¬ç§‘åŒå­¦å¯éšæ—¶è¿›ç»„ç§‘ç ”å®ä¹ ï¼Œå…¨æ–¹ä½æŒ‡å¯¼ç§‘ç ”å…¥é—¨ï¼Œæœ‰æ„ä¿ç ”çš„å¤§äºŒ/å¤§ä¸‰åŒå­¦è¯·å°½æ—©è”ç³»</strong>
<!-- <br />
<a href='https://mp.weixin.qq.com/s/6Cguf6p2ucyk-yKk5Tv1_Q' target="_blank">æ¬¢è¿é€‰æ‹©ä¿ç ”åŒ—äº¬å¸ˆèŒƒå¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢ï¼</a> -->


<div class='text-box' id="recruitment">
<pre xml:space="preserve">
æ‹›æ”¶ç¡•å£«/åšå£«ç ”ç©¶ç”Ÿï¼Œ
ç ”ç©¶æ–¹å‘ï¼šä¸‰ç»´è§†è§‰ã€å›¾å½¢å­¦ã€å…·èº«æ™ºèƒ½ã€ä¸‰ç»´æ•°å­—äººä¸äººå½¢æ™ºèƒ½ä½“ï¼›
éšæ—¶æ¬¢è¿äº†è§£å’¨è¯¢ï¼Œè¯·å°½å¯èƒ½æå‰çº¿ä¸‹/è¿œç¨‹å®ä¹ ï¼›
ç”±äºåé¢æœ‰é™ï¼Œæœ‰æ„é€šè¿‡å¤ä»¤è¥/ä¿ç ”/è€ƒç ”ç­‰æ–¹å¼è¿›ç»„çš„åŒå­¦ã€å‡éœ€è¦æå‰è”ç³»ã€‘ï¼›

ç”±äºç”³è¯·é‚®ä»¶è¾ƒå¤šï¼ŒæŠ±æ­‰æ— æ³•é€ä¸€å›å¤ï¼Œç¬¦åˆæ¡ä»¶çš„å€™é€‰åŒå­¦ä¼šåœ¨ä¸‰å¤©å†…æ”¶åˆ°å›å¤é‚®ä»¶ä»¥è¿›ä¸€æ­¥è”ç³»ï¼›
è¿‘ä¸¤å¹´æƒ…å†µçš„å‚è€ƒç”³è¯·æ¡ä»¶ï¼š
ä¿ç ”ï¼šæœ‰CCF-Aç±»è®ºæ–‡å·²å‘è¡¨/å½•ç”¨æˆ–åœ¨æŠ•ï¼ˆéœ€é™„ä¸Šè®ºæ–‡PDFï¼‰
è€ƒç ”ï¼šæš‚æ— å‚è€ƒæ¡ä»¶ï¼ˆåé¢ä¸»è¦ä¸ºä¸“ä¸šç¡•å£«ï¼‰
åšå£«ï¼šæœ‰CCF-Aç±»è®ºæ–‡å·²å‘è¡¨/å½•ç”¨
</pre>
</div>


<span class='anchor' id='-research'></span>
# ğŸ“š Research 

<a href='https://github.com/HongwenZhang' target='_blank'><img src='https://img.shields.io/github/stars/HongwenZhang?label=GitHub%20Stars&style=social'></a>

<a href='https://scholar.google.com/citations?user=6z0m_ZMAAAAJ&hl=en' target='_blank'><img src='https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Fzhang-hongwen%2Fzhang-hongwen.github.io@gs-data%2Fgs_data_shieldsio.json&style=social&label=Google%20Scholar%20Citations'></a>

<span class='anchor' id='-motion-capture'></span>

ğŸ‰&ensp; 6 paper accepted to <strong style="color:red;">TPAMI/CVPR/ICCV 2025</strong>

ğŸ‰&ensp; 1 paper accepted to <strong style="color:red;">SIGGRAPH 2024</strong>

ğŸ‰&ensp; 7 papers accepted to <strong style="color:red;">CVPR 2024</strong>

## ğŸš© Preprints
<div style="clear: both;">

   <ul>

   <li>
   <a href="https://arxiv.org/abs/2412.16212" target="_blank"><strong>ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping</strong> </a><br />
   Youxin Pang, Ruizhi Shao, Jiajun Zhang, Hanzhang Tu, Yun Liu, Boyao Zhou, <strong>Hongwen Zhang</strong>, Yebin Liu<br />
   accepted to CVPR 2025 highlight<br />
   <a href="https://arxiv.org/abs/2412.16212" target="_blank">pdf</a> 
   </li>

   <li>
   <a href="https://chaiying1.github.io/GAF.github.io/project_page" target="_blank"><strong>GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation</strong> </a><br />
   Ying Chai*, Litao Deng*, Ruizhi Shao, Jiajun Zhang, Liangjun Xing, <strong>Hongwen Zhang</strong>, Yebin Liu<br />
   arXiv 2025<br />
   <a href="https://chaiying1.github.io/GAF.github.io/project_page" target="_blank">project page</a> /
   <a href="https://arxiv.org/pdf/2506.14135" target="_blank">pdf</a> 
   </li>

   <li>
   <a href="https://arxiv.org/pdf/2405.20330v2" target="_blank"><strong>4DHands: Reconstructing Interactive Hands in 4D with Transformers</strong> </a><br />
   Dixuan Lin, Yuxiang Zhang, Mengcheng Li, Yebin Liu, Wei Jing, Qi Yan, Qianying Wang, <strong>Hongwen Zhang</strong><br />
   arXiv 2024<br />
   <a href="https://4dhands.github.io" target="_blank">project page</a> /
   <a href="https://arxiv.org/pdf/2405.20330v2" target="_blank">pdf</a> /
   <a href="https://github.com/LinDixuan/OmniHands" target="_blank">code</a>
   </li>

   <li>
   <a href="https://li-ronghui.github.io/lodgepp" target="_blank"><strong>Lodge++: High-quality and Long Dance Generation with Vivid Choreography Patterns</strong> </a><br />
   Ronghui Li, <strong>Hongwen Zhang</strong>, Yachao Zhang, Yuxiang Zhang, Youliang Zhang, Jie Guo, Yan Zhang, Xiu Li, Yebin Liu<br />
   arXiv 2024<br />
   <a href="https://li-ronghui.github.io/lodgepp" target="_blank">project page</a> /
   <a href="https://arxiv.org/abs/2410.20389" target="_blank">pdf</a>
   </li>

   </ul>

</div>

<h2> <a href='index_.html'>  [Sort by Year]</a> <a href='publications.html'>  [Full List]</a> <a href='https://github.com/HongwenZhang' target="_blank">  [Code List]</a><br /></h2>

<div style="display: flex;" class="tab-container" id="research-tab-container">
    <button class="highlight_research_tab" onclick="toggle_research_vis(&#39;#-motion-capture&#39;, this)"  id="tab_a">Motion Capture</button>
    <button class="research_tab" onclick="toggle_research_vis(&#39;#-motion-understanding&#39;, this); toggleContent()"  id="tab_b">Motion Generation</button>
    <button class="research_tab" onclick="toggle_research_vis(&#39;#-human-reconstruction&#39;, this); toggleContent()"  id="tab_c">Human Reconcstruction</button>
    <button class="research_tab" onclick="toggle_research_vis(&#39;#-animatable-avatar&#39;, this); toggleContent()"  id="tab_d">Animatable Avatar</button>
<!--     <div class="highlight_research_tab" onclick="toggle_research_vis(&#39;#-motion-capture&#39;, this)"  id="tab_a">Motion Capture</div>
    <div class="research_tab" onclick="toggle_research_vis(&#39;#-motion-understanding&#39;, this); toggleContent()"  id="tab_b">Motion Generation</div>
    <div class="research_tab" onclick="toggle_research_vis(&#39;#-human-reconstruction&#39;, this); toggleContent()"  id="tab_c">Human Reconcstruction</div>
    <div class="research_tab" onclick="toggle_research_vis(&#39;#-animatable-avatar&#39;, this); toggleContent()"  id="tab_d">Animatable Avatar</div> -->
</div>
<div class="placeholder" id="all_tab_place"></div>


<h2 id="-motion-capture">ğŸš© Human/Hand/Face/Full-body Motion Capture</h2>


<div class='paper-box' id="ProxyCap"><div class='paper-box-image'><div><img src='images/proxycap.gif' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" href="https://zhangyux15.github.io/ProxyCapV2" target="_blank"><strong>ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning</strong></a><br />
    Yuxiang Zhang, <strong>Hongwen Zhang</strong>, Liangxiao Hu, Jiajun Zhang, Hongwei Yi, Shengping Zhang, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024<br />
    <a href="https://zhangyux15.github.io/ProxyCapV2" target="_blank">project page</a> /
    <a href="https://zhangyux15.github.io/ProxyCapV2" target="_blank">pdf</a> /
    <a shape="rect" href="javascript:togglebib('ProxyCap')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{zhang2024proxycap,
  title={ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning},
  author={Zhang, Yuxiang and Zhang, Hongwen and Hu, Liangxiao and Zhang, Jiajun and Yi, Hongwei and Zhang, Shengping and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2024}
}
</pre>
</div>
</div>


<div class='paper-box' id="hmrsurvey2022"><div class='paper-box-image'><div><img src='images/hmrsurvey.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'hmrsurvey2022']);" href="https://github.com/tinatiansjz/hmr-survey" target="_blank"><strong>Recovering 3D Human Mesh from Monocular Images: A Survey</strong></a><br />
Yating Tian\*, <strong>Hongwen Zhang</strong>\*, Yebin Liu, Limin Wang<br />
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023<br />
 \* co-first authors<br /> 
<a href="https://github.com/tinatiansjz/hmr-survey" target="_blank" onclick="_gaq.push(['_trackEvent', 'hmrsurvey2022']);">project page</a> /
<a href="https://arxiv.org/pdf/2203.01923.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'hmrsurvey2022']);">pdf</a> /
<a href="https://zhanghongwen.cn/pdf/hmr_survey_v6.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'hmrsurvey2022']);">updated: v6</a> /
<a shape="rect" href="javascript:togglebib('hmrsurvey2022')" class="togglebib">bibtex</a> /
<span class='show_citations' data='6z0m_ZMAAAAJ:hMod-77fHWUC'></span>
<pre xml:space="preserve">
@article{tian2023hmrsurvey,
  title={Recovering 3D Human Mesh from Monocular Images: A Survey},
  author={Tian, Yating and Zhang, Hongwen and Liu, Yebin and Wang, Limin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023}
}
</pre>
</div>
</div>


<div class='paper-box' id="pymafx2022"><div class='paper-box-image'><div><img src='https://liuyebin.com/pymaf-x/files/mini-demo.gif' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'pymafx2022']);" href="https://www.liuyebin.com/pymaf-x" target="_blank"><strong>PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images</strong></a><br />
<strong>Hongwen Zhang</strong>, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, Yebin Liu<br />
    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023<br />
    <em style="color:red;">a framework for human/hand/face/full-body model reconstruction</em><br />
    <a href="https://www.liuyebin.com/pymaf-x" target="_blank" onclick="_gaq.push(['_trackEvent', 'pymafx2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2207.06400.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'pymafx2022']);">pdf</a> /
    <a href="https://www.bilibili.com/video/BV1pN4y1T7dY" target="_blank" onclick="_gaq.push(['_trackEvent', 'pymafx2022']);">video</a> /
    <a href="https://github.com/HongwenZhang/PyMAF-X" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'pymafx2022']);">code</a> /
<a shape="rect" href="javascript:togglebib('pymafx2022')" class="togglebib">bibtex</a> /
<span class='show_citations' data='6z0m_ZMAAAAJ:7PzlFSSx8tAC'></span>
<pre xml:space="preserve">
@article{pymafx2023,
  title={PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images},
  author={Zhang, Hongwen and Tian, Yating and Zhang, Yuxiang and Li, Mengcheng and An, Liang and Sun, Zhenan and Liu, Yebin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023}
}
    </pre>
</div>
</div>


<div class='paper-box' id="zhang2020learning"><div class='paper-box-image'><div><img src='images/densepose2smpl_demo.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2020learning']);" href="DensePose2SMPL" target="_blank"><strong>Learning 3D Human Shape and Pose from Dense Body Parts</strong></a><br />
    <strong>Hongwen Zhang</strong>, Jie Cao, Guo Lu, Wanli Ouyang, Zhenan Sun<br />
    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020<br />
    <em style="color:red;">DensePose to SMPL; part-based regressor; more robust to occlusion</em><br />
    <a href="DensePose2SMPL" target="_blank" onclick="_gaq.push(['_trackEvent', 'zhang2020learning']);">project page</a> /
    <a href="https://arxiv.org/pdf/1912.13344.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2020learning']);">pdf</a> /
    <a href="https://github.com/HongwenZhang/DaNet-DensePose2SMPL" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'zhang2020learning']);">code</a> /
    <a shape="rect" href="javascript:togglebib('zhang2020learning')" class="togglebib">bibtex</a> /
<span class='show_citations' data='6z0m_ZMAAAAJ:mB3voiENLucC'></span>
<pre xml:space="preserve">
@article{zhang2020densepose2smpl,
  title={Learning 3D Human Shape and Pose from Dense Body Parts},
  author={Zhang, Hongwen and Cao, Jie and Lu, Guo and Ouyang, Wanli and Sun, Zhenan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={5},
  pages={2610--2627},
  year={2022},
}
    </pre>
</div>
</div>


<div class="content" onclick="expandContent()" id='content_mask'>


<div class='paper-box' id="pymaf2021"><div class='paper-box-image'><div><img src='https://liuyebin.com/pymaf-x/files/flashmob.gif' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'pymaf2021']);" href="pymaf" target="_blank"><strong>PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</strong></a><br />
    <strong>Hongwen Zhang</strong>\*, Yating Tian\*, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, Zhenan Sun<br />
    IEEE International Conference on Computer Vision (ICCV), 2021 <strong style="color:red;">â˜† Oral Paper</strong><br />
    * Equal contribution<br />
    <a href="pymaf" target="_blank" onclick="_gaq.push(['_trackEvent', 'pymaf2021']);">project page</a> /
    <a href="https://arxiv.org/pdf/2103.16507.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'pymaf2021']);">pdf</a> /
    <a href="https://github.com/HongwenZhang/PyMAF" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'pymaf2021']);">code</a> /
    <a href="https://www.youtube.com/watch?v=yqEmznSKjYI" target="_blank" onclick="_gaq.push(['_trackEvent', 'pymaf2021']);">video</a> /
    <a shape="rect" href="javascript:togglebib('pymaf2021')" class="togglebib">bibtex</a> /
<span class='show_citations' data='6z0m_ZMAAAAJ:Zph67rFs4hoC'></span>
<pre xml:space="preserve">
@inproceedings{pymaf2021,
  title={PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop},
  author={Zhang, Hongwen and Tian, Yating and Zhou, Xinchi and Ouyang, Wanli and Liu, Yebin and Wang, Limin and Sun, Zhenan},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  year={2021}
}
    </pre>
</div>
</div>


<div class='paper-box' id="intaghand2022"><div class='paper-box-image'><div><img src='images/intaghand_demo.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'intaghand2022']);" href="http://www.liuyebin.com/IntagHand/Intaghand.html" target="_blank"><strong>Interacting Attention Graph for Single Image Two-Hand Reconstruction</strong></a><br />
    Mengcheng Li, Liang An, <strong>Hongwen Zhang</strong>, Lianpeng Wu, Feng Chen, Tao Yu, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022 <strong style="color:red;">â˜† Oral Paper</strong><br />
    <a href="http://www.liuyebin.com/IntagHand/Intaghand.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'intaghand2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2203.09364.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'intaghand2022']);">pdf</a> /
    <a href="https://github.com/Dw1010/IntagHand" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'intaghand2022']);">code</a> /
    <a shape="rect" href="javascript:togglebib('intaghand2022')" class="togglebib">bibtex</a> /
    <span class='show_citations' data='6z0m_ZMAAAAJ:qxL8FJ1GzNcC'></span>
<pre xml:space="preserve">
@inproceedings{li2022interacting,
  title={Interacting Attention Graph for Single Image Two-Hand Reconstruction},
  author={Li, Mengcheng and An, Liang and Zhang, Hongwen and Wu, Lianpeng and Chen, Feng and Yu, Tao and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
    </pre>
</div>
</div>


<div class='paper-box' id="CHOI"><div class='paper-box-image'><div><img src='images/CHOI.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" href="https://junxinghu.github.io/projects/hoi.html" target="_blank"><strong>Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images</strong> </a><br />
    Junxing Hu, <strong>Hongwen Zhang</strong>, Zerui Chen, Mengcheng Li, Yunlong Wang, Yebin Liu, Zhenan Sun<br />
    AAAI Conference on Artificial Intelligence (AAAI), 2024<br />
    <a href="https://junxinghu.github.io/projects/hoi.html" target="_blank">project page</a> /
    <a href="https://arxiv.org/abs/2305.20089" target="_blank">pdf</a> /
    <a href="https://github.com/JunxingHu/CHOI" target="_blank">code</a> /
    <a shape="rect" href="javascript:togglebib('CHOI')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{hu2024choi,
  title={Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images},
  author={Hu, Junxing and Zhang, Hongwen and Chen, Zerui and Li, Mengcheng and Wang, Yunlong and Liu, Yebin and Sun, Zhenan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024},
}
</pre>
</div>
</div>


<div class='paper-box' id="jia2023delving"><div class='paper-box-image'><div><img src='images/paff.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'jia2023delving']);" href="https://kairobo.github.io/PaFF" target="_blank"><strong>Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery</strong></a><br />
    Kai Jia, <strong>Hongwen Zhang</strong>, Liang An, Yebin Liu<br />
    AAAI Conference on Artificial Intelligence (AAAI), 2023<br />
    <a href="https://kairobo.github.io/PaFF" target="_blank" onclick="_gaq.push(['_trackEvent', 'jia2023delving']);">project page</a> /
    <a href="https://arxiv.org/pdf/2301.06020.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'jia2023delving']);">pdf</a> /
    <a href="https://github.com/Kairobo/PaFF" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'jia2023delving']);">code</a> /
    <a shape="rect" href="javascript:togglebib('jia2023delving')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{jia2023delving,
  title={Delving Deep into Pixel Alignment Feature for Accurate Multi-view Human Mesh Recovery},
  author={Jia, Kai and Zhang, Hongwen and An, Liang and Liu, Yebin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2023},
}
    </pre>
</div>
</div>


<div class='paper-box' id="lightcap2022"><div class='paper-box-image'><div><img src='https://liuyebin.com/realtime_cap_project/realtotalcap/realtotalcap.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'lightcap2022']);" href="http://www.liuyebin.com/realtime_cap_project/realtotalcap/realtotalcap.html" target="_blank"><strong>Real-time Sparse-view Multi-person Total Motion Capture</strong></a><br />
    Yuxiang Zhang, Zeping Ren, Liang An, <strong>Hongwen Zhang</strong>, Tao Yu, Yebin Liu<br />
    <a href="http://www.liuyebin.com/realtime_cap_project/realtotalcap/realtotalcap.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'lightcap2022']);">project page</a> /
    <a shape="rect" href="javascript:togglebib('lightcap2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
coming
    </pre>
</div>
</div>


<div class='paper-box' id="zhang2019danet"><div class='paper-box-image'><div><img src='images/danet_part.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2019danet']);" href="DensePose2SMPL" target="_blank"><strong>DaNet: Decompose-and-aggregate Network for 3D Human Shape and Pose Estimation</strong></a><br />
    <strong>Hongwen Zhang</strong> et al.<br />
    ACM International Conference on Multimedia (MM), 2019<br />
    <em style="color:red;">part-based method for human mesh recovery</em><br />
    <a href="DensePose2SMPL" target="_blank" onclick="_gaq.push(['_trackEvent', 'zhang2019danet']);">project page</a> /
    <a href="pdf/acmmm19DaNet.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2019danet']);">pdf</a> /
    <a href="https://github.com/HongwenZhang/DaNet-DensePose2SMPL" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'zhang2019danet']);">code</a> /
<a shape="rect" href="javascript:togglebib('zhang2019danet')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{zhang2019danet,
  title={DaNet: Decompose-and-aggregate Network for 3D Human Shape and Pose Estimation},
  author={Zhang, Hongwen and Cao, Jie and Lu, Guo and Ouyang, Wanli and Sun, Zhenan},
  booktitle={Proceedings of the 27th ACM International Conference on Multimedia},
  pages={935--944},
  year={2019},
  organization={ACM}
}
    </pre>
</div>
</div>

<div class="placeholder" id="tab_b_place"></div>


<div class='paper-box' id="zhang2019adversarial"><div class='paper-box-image'><div><img src='images/JVCR.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2019adversarial']);" href="https://github.com/HongwenZhang/JVCR-3Dlandmark" target="_blank"><strong>Adversarial Learning Semantic Volume for 2D/3D Face Shape Regression in the Wild</strong></a><br />
    <strong>Hongwen Zhang</strong> et al.<br />
    IEEE Transactions on Image Processing (JCR-Q1, CCF-A), 2019<br />
    <a href="https://openreview.net/pdf?id=gafjGfv8uR" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2019adversarial']);">pdf</a> /
    <a href="https://github.com/HongwenZhang/JVCR-3Dlandmark" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'zhang2019adversarial']);">code (â˜† Stars: 150+)</a> /
    <a shape="rect" href="javascript:togglebib('zhang2019adversarial')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@article{zhang2019adversarial,
  title={Adversarial Learning Semantic Volume for 2D/3D Face Shape Regression in the Wild},
  author={Zhang, Hongwen and Li, Qi and Sun, Zhenan},
  journal={IEEE Transactions on Image Processing},
  volume={28},
  number={9},
  pages={4526--4540},
  year={2019},
  publisher={IEEE}
}
    </pre>
</div>
</div>

<span class='anchor' id='-motion-understanding'></span>


<div class='paper-box' id="zhang2018combining"><div class='paper-box-image'><div><img src='images/ECT.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2018combining']);" href="https://github.com/HongwenZhang/ECT-FaceAlignment" target="_blank"><strong>Combining Data-driven and Model-driven Methods for Robust Facial Landmark Detection</strong></a><br />
    <strong>Hongwen Zhang</strong> et al.<br />
    IEEE Transactions on Information Forensics and Security (JCR-Q1, CCF-A), 2018<br />
    <a href="https://arxiv.org/pdf/1611.10152.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'zhang2018combining']);">pdf</a> /
    <a href="https://github.com/HongwenZhang/ECT-FaceAlignment" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'zhang2018combining']);">code</a> /
    <a shape="rect" href="javascript:togglebib('zhang2018combining')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@article{zhang2018combining,
  title={Combining data-driven and model-driven methods for robust facial landmark detection},
  author={Zhang, Hongwen and Li, Qi and Sun, Zhenan and Liu, Yunfan},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={13},
  number={10},
  pages={2409--2422},
  year={2018},
  publisher={IEEE}
}
    </pre>
</div>
</div>




<h2 id="-motion-understanding">ğŸš© Motion Understanding and Generation</h2>


<div class='paper-box' id="liu2020disentangling"><div class='paper-box-image'><div><img src='images/ms-g3d.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'liu2020disentangling']);" href="https://arxiv.org/abs/2003.14111" target="_blank"><strong>Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</strong></a><br />
    Ziyu Liu, <strong>Hongwen Zhang</strong>, Zhenghao Chen, Zhiyong Wang, Wanli Ouyang<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 <strong style="color:red;">â˜† Oral Paper</strong><br />
    <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Disentangling_and_Unifying_Graph_Convolutions_for_Skeleton-Based_Action_Recognition_CVPR_2020_paper.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'liu2020disentangling']);">pdf</a> /
    <a href="https://www.youtube.com/watch?v=Vm024c-tYIE" target="_blank" onclick="_gaq.push(['_trackEvent', 'Slides', 'Download', 'liu2020disentangling']);">presentation</a> /
    <a href="https://github.com/kenziyuliu/MS-G3D" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'liu2020disentangling']);">code</a> /
    <a shape="rect" href="javascript:togglebib('liu2020disentangling')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{liu2020disentangling,
  title={Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition},
  author={Liu, Ziyu and Zhang, Hongwen and Chen, Zhenghao and Wang, Zhiyong and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2020}
}
    </pre>
</div>
</div>


<div class='paper-box' id="HHMR"><div class='paper-box-image'><div><img src='images/HHMR.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" href="https://www.liuyebin.com/HHMR/HHMR.html" target="_blank"><strong>HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models</strong> </a><br />
   Mengcheng Li, <strong>Hongwen Zhang</strong>, Yuxiang Zhang, Ruizhi Shao, Tao Yu, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024 <strong style="color:red;">â˜† Highlight Paper</strong><br />
    <a href="https://www.liuyebin.com/HHMR/HHMR.html" target="_blank">project page</a> /
    <a href="https://www.liuyebin.com/HHMR/HHMR.html" target="_blank">pdf</a> /
    <a shape="rect" href="javascript:togglebib('HHMR')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{li2024hhmr,
  title={HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models},
  author={Li, Mengcheng and Zhang, Hongwen and Zhang, Yuxiang and Shao, Ruizhi and Yu, Tao and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2024}
}
</pre>
</div>
</div>


<div class="placeholder" id="tab_c_place"></div>


<div class='paper-box' id="Lodge"><div class='paper-box-image'><div><img src='https://liuyebin.com/arXiv23/lodge.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" href="https://li-ronghui.github.io/lodge" target="_blank"><strong>Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives</strong> </a><br />
   Ronghui Li, Yuxiang Zhang, Yachao Zhang, <strong>Hongwen Zhang</strong>, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024<br />
    <a href="https://li-ronghui.github.io/lodge" target="_blank">project page</a> /
    <a href="https://li-ronghui.github.io/lodge" target="_blank">pdf</a> /
    <a href="https://github.com/li-ronghui/LODGE" target="_blank">code</a> /
    <a shape="rect" href="javascript:togglebib('Lodge')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{li2024lodge,
  title={Lodge: A coarse to fine diffusion network for long dance generation guided by the characteristic dance primitives},
  author={Li, Ronghui and Zhang, YuXiang and Zhang, Yachao and Zhang, Hongwen and Guo, Jie and Zhang, Yan and Liu, Yebin and Li, Xiu},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2024}
}
</pre>
</div>
</div>

<span class='anchor' id='-human-reconstruction'></span>


<div class='paper-box' id="Narrator"><div class='paper-box-image'><div><img src='images/narrator.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'Narrator']);" href="http://cic.tju.edu.cn/faculty/likun/projects/Narrator/index.html" target="_blank"><strong>Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning</strong></a><br />
    Haibiao Xuan, Xiongzheng Li, Jinsong Zhang, <strong>Hongwen Zhang</strong>, Yebin Liu, Kun Li<br />
    IEEE International Conference on Computer Vision (ICCV), 2023<br />
    <a href="http://cic.tju.edu.cn/faculty/likun/projects/Narrator/index.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'Narrator']);">project page</a> /
    <a href="https://arxiv.org/pdf/2303.09410.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'Narrator']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('Narrator')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@InProceedings{xuan2023narrator,
title={Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning},
author={Xuan, Haibiao and Li, Xiongzheng and Zhang, Jinsong and Zhang, Hongwen and Liu, Yebin and Li, Kun},
booktitle={IEEE International Conference on Computer Vision},
year={2023}
}
</pre>
</div>
</div>



<h2 id="-human-reconstruction">ğŸš© Clothed Human Reconstruction/Rendering</h2>



<div class='paper-box' id="dbfield2022"><div class='paper-box-image'><div><img src='images/dbfield.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'dbfield2022']);" href="http://www.liuyebin.com/dbfield/dbfield.html" target="_blank"><strong>DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering</strong></a><br />
    Ruizhi Shao, <strong>Hongwen Zhang</strong>, He Zhang, Mingjia Chen, Yanpei Cao, Tao Yu, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022<br />
    <a href="http://www.liuyebin.com/dbfield/dbfield.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'dbfield2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2106.03798.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'dbfield2022']);">pdf</a> /
    <a href="https://github.com/DSaurus/DoubleField" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'dbfield2022']);">code</a> /
    <a shape="rect" href="javascript:togglebib('dbfield2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{shao2022doublefield,
  title={DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering},
  author={Shao, Ruizhi and Zhang, Hongwen and Zhang, He and Chen, Mingjia and Cao, Yanpei and Yu, Tao and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
    </pre>
</div>
</div>


<div class='paper-box' id="diffst2022"><div class='paper-box-image'><div><img src='https://liuyebin.com/eccv22teaser/diffustereo.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'diffst2022']);" href="https://liuyebin.com" target="_blank"><strong>DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</strong></a><br />
    Ruizhi Shao, Zerong Zheng, <strong>Hongwen Zhang</strong>, Jingxiang Sun, Yebin Liu<br />
    European Conference on Computer Vision (ECCV), 2022 <strong style="color:red;">â˜† Oral Paper</strong><br />
    <a href="http://www.liuyebin.com" target="_blank" onclick="_gaq.push(['_trackEvent', 'diffst2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2207.08000.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'diffst2022']);">pdf</a> /
    <a href="https://github.com/DSaurus/DiffuStereo" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'diffst2022']);">code</a> /
    <a shape="rect" href="javascript:togglebib('diffst2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{shao2022diffusionstereo,
  title={DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras},
  author={Shao, Ruizhi and Zheng, Zerong and Zhang, Hongwen and Sun, Jingxiang and Liu, Yebin},
  booktitle={European Conference on Computer Vision},
  year={2022}
}
    </pre>
</div>
</div>

<div class="placeholder" id="tab_d_place"></div>


<div class='paper-box' id="floren2022"><div class='paper-box-image'><div><img src='https://liuyebin.com/siggraph22/floren.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'floren2022']);" href="https://liuyebin.com" target="_blank"><strong>FloRen: Real-time High-quality Human Performance Rendering via Appearance Flow Using Sparse RGB Cameras</strong></a><br />
    Ruizhi Shao, Liliang Chen, Zerong Zheng, <strong>Hongwen Zhang</strong>, Yuxiang Zhang, Han Huang, Yandong Guo, Yebin Liu<br />
    SIGGRAPH Asia Conference Papers, 2022<br />
    <a href="http://www.liuyebin.com" target="_blank" onclick="_gaq.push(['_trackEvent', 'floren2022']);">project page</a> /
    <a href="https://dl.acm.org/doi/abs/10.1145/3550469.3555409" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'floren2022']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('floren2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{shao2022floren,
  title={FloRen: Real-time High-quality Human Performance Rendering via Appearance Flow Using Sparse RGB Cameras},
  author={Shao, Ruizhi and Chen, Liliang and Zheng, Zerong and Zhang, Hongwen and Zhang, Yuxiang and Huang, Han and Guo, Yandong and Liu, Yebin},
  booktitle={SIGGRAPH Asia Conference Papers},
  year={2022}
}
    </pre>
</div>
</div>

<span class='anchor' id='-animatable-avatar'></span>


<div class='paper-box' id="tensor4d"><div class='paper-box-image'><div><img src='https://liuyebin.com/arxiv/tensor4d.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'tensor4d']);" href="https://liuyebin.com/tensor4d/tensor4d.html" target="_blank"><strong>Tensor4D: Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering</strong></a><br />
    Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, <strong>Hongwen Zhang</strong>, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023 <strong style="color:red;">â˜† Highlight</strong><br />
    <a href="https://liuyebin.com/tensor4d/tensor4d.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'tensor4d']);">project page</a> /
    <a href="https://arxiv.org/pdf/2211.11610.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'tensor4d']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('tensor4d')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{shao2023tensor4d,
  title={Tensor4D: Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering},
  author={Shao, Ruizhi and Zheng, Zerong and Tu, Hanzhang and Liu, Boning and Zhang, Hongwen and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
</pre>
</div>
</div>


<span class='anchor' id='-cloth-modeling'></span>


<h2 id="-cloth-modeling">ğŸš© Animatable Clothed Human Modeling</h2>


<div class='paper-box' id="closet2023"><div class='paper-box-image'><div><img src='https://liuyebin.com/cvpr23/closet.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'closet2023']);" href="https://www.liuyebin.com/closet" target="_blank"><strong>CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition</strong></a><br />
    <strong>Hongwen Zhang</strong>, Siyou Lin, Ruizhi Shao, Yuxiang Zhang, Zerong Zheng, Han Huang, Yandong Guo, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023<br />
    <a href="https://www.liuyebin.com/closet" target="_blank" onclick="_gaq.push(['_trackEvent', 'closet2023']);">project page</a> /
    <a href="https://arxiv.org/abs/2304.03167" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'closet2023']);">pdf</a> /
    <a href="https://github.com/HongwenZhang/CloSET" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'closet2023']);">code (coming)</a> /
    <a href="https://github.com/HongwenZhang/THuman-CloSET" target="_blank" onclick="_gaq.push(['_trackEvent', 'Data', 'Download', 'closet2023']);">dataset (available)</a> /
    <a shape="rect" href="javascript:togglebib('closet2023')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{zhang2023closet,
  title={CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition},
  author={Zhang, Hongwen and Lin, Siyou and Shao, Ruizhi and Zhang, Yuxiang and Zheng, Zerong and Huang, Han and Guo, Yandong and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
</pre>
</div>
</div>


<div class='paper-box' id="fite2022"><div class='paper-box-image'><div><img src='https://liuyebin.com/eccv22teaser/fite.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">

<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'fite2022']);" href="https://jsnln.github.io/fite" target="_blank"><strong>Learning Implicit Templates for Point-Based Clothed Human Modeling</strong></a><br />
    Siyou Lin, <strong>Hongwen Zhang</strong>, Zerong Zheng, Ruizhi Shao, Yebin Liu<br />
    European Conference on Computer Vision (ECCV), 2022<br />
    <a href="https://jsnln.github.io/fite" target="_blank" onclick="_gaq.push(['_trackEvent', 'fite2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2207.06955.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'fite2022']);">pdf</a> /
    <a href="https://github.com/jsnln/fite" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'fite2022']);">code</a> /
    <a shape="rect" href="javascript:togglebib('fite2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{lin2022fite,
  title={Learning Implicit Templates for Point-Based Clothed Human Modeling},
  author={Lin, Siyou and Zhang, Hongwen and Zheng, Zerong and Shao, Ruizhi and Liu, Yebin},
  booktitle={European Conference on Computer Vision},
  year={2022}
}
    </pre>

</div>
</div>


<div class='paper-box' id="Intrinsic"><div class='paper-box-image'><div><img src='images/IntrinsicCloth.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'Intrinsic']);" href="https://jsnln.github.io/iccv2023_intrinsic/index.html" target="_blank"><strong>Leveraging Intrinsic Properties for Non-Rigid Garment Alignment</strong></a><br />
    Siyou Lin, Boyao Zhou, Zerong Zheng, <strong>Hongwen Zhang</strong>, Yebin Liu<br />
    IEEE International Conference on Computer Vision (ICCV), 2023<br />
    <a href="https://jsnln.github.io/iccv2023_intrinsic/index.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'Intrinsic']);">project page</a> /
    <a href="https://arxiv.org/pdf/2308.09519.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'Intrinsic']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('Intrinsic')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@InProceedings{lin2023leveraging,
title={Leveraging Intrinsic Properties for Non-Rigid Garment Alignment},
author={Lin, Siyou and Zhou, Boyao and Zheng, Zerong and Zhang, Hongwen and Liu, Yebin},
booktitle={IEEE International Conference on Computer Vision},
year={2023}
}
</pre>

</div>
</div>




<div class='paper-box' id="CaPhy"><div class='paper-box-image'><div><img src='images/CaPhy.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'CaPhy']);" href="https://suzhaoqi.github.io/projects/CaPhy" target="_blank"><strong>CaPhy: Capturing Physical Properties for Animatable Human Avatars</strong></a><br />
    Zhaoqi Su, Liangxiao Hu, Siyou Lin, <strong>Hongwen Zhang</strong>, Shengping Zhang, Justus Thies, Yebin Liu<br />
    IEEE International Conference on Computer Vision (ICCV), 2023<br />
    <a href="https://suzhaoqi.github.io/projects/CaPhy" target="_blank" onclick="_gaq.push(['_trackEvent', 'CaPhy']);">project page</a> /
    <a href="https://arxiv.org/pdf/2308.05925.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'CaPhy']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('CaPhy')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@InProceedings{su2023caphy,
title={CaPhy: Capturing Physical Properties for Animatable Human Avatars},
author={Su, Zhaoqi and Hu, Liangxiao and Lin, Siyou and Zhang, Hongwen and Zhang, Shengping and Thies, Justus and Liu, Yebin},
booktitle={IEEE International Conference on Computer Vision},
year={2023}
}
</pre>
</div>
</div>



<h2 id="-animatable-avatar">ğŸš© Animatable Avatar</h2>


<div class='paper-box' id="GaussianAvatar"><div class='paper-box-image'><div><img src='images/GaussianAvatar.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" href="https://huliangxiao.github.io/GaussianAvatar" target="_blank"><strong>GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</strong> </a><br />
   Liangxiao Hu, <strong>Hongwen Zhang</strong>, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024<br />
    <a href="https://huliangxiao.github.io/GaussianAvatar" target="_blank">project page</a> /
    <a href="https://huliangxiao.github.io/GaussianAvatar" target="_blank">pdf</a> /
    <a href="https://github.com/huliangxiao/GaussianAvatar" target="_blank">code</a> /
    <a shape="rect" href="javascript:togglebib('GaussianAvatar')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{hu2024gaussianavatar,
  title={GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians},
  author={Hu, Liangxiao and Zhang, Hongwen and Zhang, Yuxiang and Zhou, Boyao and Liu, Boning and Zhang, Shengping and Nie, Liqiang},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2024}
}
</pre>
</div>
</div>


<div class='paper-box' id="LatentAvatar"><div class='paper-box-image'><div><img src='https://liuyebin.com/siggraph23/latentAvatar.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'LatentAvatar']);" href="https://liuyebin.com/latentavatar" target="_blank"><strong>LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar</strong></a><br />
    Yuelang Xu, <strong>Hongwen Zhang</strong>, Lizhen Wang, Xiaochen Zhao, Han Huang, Guojun Qi, Yebin Liu<br />
    SIGGRAPH Conference Proceedings, 2023<br />
    <a href="https://liuyebin.com/latentavatar" target="_blank" onclick="_gaq.push(['_trackEvent', 'LatentAvatar']);">project page</a> /
    <a href="https://arxiv.org/abs/2305.01190" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'LatentAvatar']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('LatentAvatar')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@InProceedings{xu2023latentavatar,
title={LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar},
author={Xu, Yuelang and Zhang, Hongwen and Wang, Lizhen and Zhao, Xiaochen and Han, Huang and Guojun, Qi and Liu, Yebin},
booktitle={ACM SIGGRAPH 2023 Conference Proceedings},
year={2023}
}
</pre>
</div>
</div>


<div class='paper-box' id="AvatarReX2023"><div class='paper-box-image'><div><img src='https://liuyebin.com/siggraph23/AvatarRex.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">

<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'AvatarReX2023']);" href="https://liuyebin.com/AvatarRex" target="_blank"><strong>AvatarReX: Real-time Expressive Full-body Avatars</strong></a><br />
    Zerong Zheng, Xiaochen Zhao, <strong>Hongwen Zhang</strong>, Boning Liu, Yebin Liu<br />
    ACM Transactions on Graphics (TOG), 2023<br />
    SIGGRAPH Journal Track<br />
    <a href="https://liuyebin.com/AvatarRex" target="_blank" onclick="_gaq.push(['_trackEvent', 'AvatarReX2023']);">project page</a> /
    <a href="https://arxiv.org/abs/2305.04789" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'AvatarReX2023']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('AvatarReX2023')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@article{zheng2023avatarrex,
title={AvatarReX: Real-time Expressive Full-body Avatars},
author={Zheng, Zerong and Zhao, Xiaochen and Zhang, Hongwen and Liu, Boning and Liu, Yebin},
journal={ACM Transactions on Graphics (TOG)},
volume={42},
number={4},
year={2023},
}
</pre>

</div>
</div>


<div class='paper-box' id="HAvatar"><div class='paper-box-image'><div><img src='https://liuyebin.com/thumbnail/havatar.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'HAvatar']);" href="http://www.liuyebin.com/havatar" target="_blank"><strong>HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural Radiance Field</strong></a><br />
    Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, <strong>Hongwen Zhang</strong>, Jinli Suo, Yebin Liu<br />
    ACM Transactions on Graphics (TOG), 2023<br />
    <a href="http://www.liuyebin.com/havatar" target="_blank" onclick="_gaq.push(['_trackEvent', 'HAvatar']);">project page</a> /
    <a href="https://arxiv.org/pdf/2309.17128.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'HAvatar']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('HAvatar')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@article{zhao2023havatar,
  title={HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural Radiance Field},
  author={Zhao, Xiaochen and Wang, Lizhen and Sun, Jingxiang and Zhang, Hongwen and Suo, Jinli and Liu, Yebin},
  journal={ACM Transactions on Graphics (TOG)},
  year={2023},
}
</pre>
</div>
</div>


<div class='paper-box' id="AvatarMAV"><div class='paper-box-image'><div><img src='https://liuyebin.com/siggraph23/AvatarMAV.jpg' alt='image' width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'AvatarMAV']);" href="https://www.liuyebin.com/avatarmav" target="_blank"><strong>AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural Voxels</strong></a><br />
    Yuelang Xu, Lizhen Wang, Xiaochen Zhao, <strong>Hongwen Zhang</strong>, Yebin Liu<br />
    SIGGRAPH Conference Proceedings, 2023<br />
    <a href="https://liuyebin.com/avatarmav" target="_blank" onclick="_gaq.push(['_trackEvent', 'AvatarMAV']);">project page</a> /
    <a href="https://arxiv.org/abs/2211.13206" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'AvatarMAV']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('AvatarMAV')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@InProceedings{xu2023avatarmav,
title={AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural Voxels},
author={Xu, Yuelang and Wang, Lizhen and Zhao, Xiaochen and Zhang, Hongwen and Liu, Yebin},
booktitle={ACM SIGGRAPH 2023 Conference Proceedings},
year={2023}
}
</pre>
</div>
</div>


<div class='paper-box' id="StyleAvatar"><div class='paper-box-image'><div><img src='https://liuyebin.com/siggraph23/styleAvatar.jpg' alt='image' width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'StyleAvatar']);" href="https://liuyebin.com/styleavatar/styleavatar.html" target="_blank"><strong>StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video</strong></a><br />
    Lizhen Wang, Xiaochen Zhao, Jingxiang Sun, Yuxiang Zhang, <strong>Hongwen Zhang</strong>, Tao Yu, and Yebin Liu.Yebin Liu<br />
    SIGGRAPH Conference Proceedings, 2023<br />
    <a href="https://liuyebin.com/styleavatar/styleavatar.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'StyleAvatar']);">project page</a> /
    <a href="https://arxiv.org/abs/2305.00942" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'StyleAvatar']);">pdf</a> /
    <a shape="rect" href="javascript:togglebib('StyleAvatar')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{wang2023styleavatar,
  title={StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video},
  author={Wang, Lizhen and Zhao, Xiaochen and Sun, Jingxiang and Zhang, Yuxiang and Zhang, Hongwen and Yu, Tao and Liu, Yebin},
  booktitle={ACM SIGGRAPH 2023 Conference Proceedings},
  year={2023}
}
</pre>
</div>
</div>



<div class='paper-box' id="next3d"><div class='paper-box-image'><div><img src='https://liuyebin.com/cvpr23/next3d.jpg' alt='image' width="100%" loading="lazy"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'next3d']);" href="https://mrtornado24.github.io/Next3D" target="_blank"><strong>Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</strong></a><br />
    Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, <strong>Hongwen Zhang</strong>, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023 <strong style="color:red;">â˜† Highlight</strong><br />
    <a href="https://mrtornado24.github.io/Next3D" target="_blank" onclick="_gaq.push(['_trackEvent', 'next3d']);">project page</a> /
    <a href="https://arxiv.org/pdf/2211.11208.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'next3d']);">pdf</a> /
    <a href="https://github.com/MrTornado24/Next3D" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'next3d']);">code</a> /
    <a shape="rect" href="javascript:togglebib('next3d')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{sun2023next3d,
  title={Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars},
  author={Sun, Jingxiang and Wang, Xuan and Wang, Lizhen and Li, Xiaoyu and Zhang, Yong and Zhang, Hongwen and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
</pre>
</div>
</div>

<div class="placeholder" id="end_place"></div>


<div class='paper-box' id="avatarcap2022"><div class='paper-box-image'><div><img src='https://liuyebin.com/eccv22teaser/avatarcap.jpg' alt='image' width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'avatarcap2022']);" href="https://liuyebin.com/avatarcap/avatarcap.html" target="_blank"><strong>AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture</strong></a><br />
    Zhe Li, Zerong Zheng, <strong>Hongwen Zhang</strong>, Chaonan Ji, Yebin Liu<br />
    European Conference on Computer Vision (ECCV), 2022<br />
    <a href="https://liuyebin.com/avatarcap/avatarcap.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'avatarcap2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2207.02031.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'avatarcap2022']);">pdf</a> /
    <a href="https://github.com/lizhe00/AvatarCap" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'avatarcap2022']);">code</a> /
    <a shape="rect" href="javascript:togglebib('avatarcap2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{li2022avatarcap,
    title={AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture},
    author={Li, Zhe and Zheng, Zerong and Zhang, Hongwen and Ji, Chaonan and Liu, Yebin},
    booktitle={European Conference on Computer Vision},
    year={2022},
}
    </pre>
</div>
</div>


<div class='paper-box' id="slrf2022"><div class='paper-box-image'><div><img src='images/slrf.jpg' alt='image' width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'slrf2022']);" href="https://liuyebin.com/slrf/slrf.html" target="_blank"><strong>Structured Local Radiance Fields for Human Avatar Modeling</strong></a><br />
    Zerong Zheng, Han Huang, Tao Yu, <strong>Hongwen Zhang</strong>, Yandong Guo, Yebin Liu<br />
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022<br />
    <a href="https://liuyebin.com/slrf/slrf.html" target="_blank" onclick="_gaq.push(['_trackEvent', 'slrf2022']);">project page</a> /
    <a href="https://arxiv.org/pdf/2203.14478.pdf" target="_blank" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'slrf2022']);">pdf</a> /
    <a href="https://github.com/ZhengZerong/THUman4.0-Dataset" target="_blank" onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'slrf2022']);">data</a> /
    <a shape="rect" href="javascript:togglebib('slrf2022')" class="togglebib">bibtex</a>
<pre xml:space="preserve">
@inproceedings{zheng2022structured,
  title={Structured Local Radiance Fields for Human Avatar Modeling},
  author={Zheng, Zerong and Huang, Han and Yu, Tao and Zhang, Hongwen and Guo, Yandong and Liu, Yebin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
    </pre>
</div>
</div>


<!-- <div class='paper-box' id="iddd"><div class='paper-box-image'><div><img src='images/500x300.jpg' alt='image' width="100%"></div></div>
<div class='paper-box-text' markdown="1">


</div>
</div> -->


<h2> <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'more']);" href="publications.html">Click to View the Full List of Publications</a></h2>
<h2> <a href='index_.html'>  [Sort by Year]</a> <a href='publications.html'>  [Full List]</a> <a href='https://github.com/HongwenZhang' target="_blank">  [Code List]</a><br /></h2>

<div class="tooltip">Click to Load More Publications</div>

</div>


<!-- <div class="more-btn" onclick="toggleContent()">Click to Load More Publications</div> -->

<div class="toggle-btn" onclick="toggleContent()">
    <div class="more-btn">Click to Load More Publications on Clothed Human/Avatar Modeling</div>
</div>


<span class='anchor' id='-honor-award'></span>

# ğŸ–ï¸ Honor & Award

<div class='paper-box' id="CAST-YESS"><div class='paper-box-image'><div><img src='images/cast_yess.jpg' alt='é’æ‰˜å·¥ç¨‹' width="100%" loading="eager"></div></div>
<div class='paper-box-text' markdown="1">
<a ><strong>ä¸­å›½ç§‘åé’å¹´äººæ‰æ‰˜ä¸¾å·¥ç¨‹, Young Elite Scientist Sponsorship Program by CAST</strong></a><br />
     China Association for Science and Technology (CAST)<br />
</div>
</div>

<div class='paper-box' id="CAS-D"><div class='paper-box-image'><div><img src='images/award-CAS.jpg' alt='ä¼˜åšè¯ä¹¦' width="100%" loading="eager"></div></div>
<div class='paper-box-text' markdown="1">
<a ><strong>ä¸­å›½ç§‘å­¦é™¢ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡, CAS Outstanding Doctoral Dissertation</strong></a><br />
    æˆäºˆå•ä½: ä¸­å›½ç§‘å­¦é™¢, Awarded by the Chinese Academy of Sciences (CAS)<br />
</div>
</div>

<span class='anchor' id='-academic-services'></span>

# ğŸ“‘ Academic Services

Reviewer for Journals: TPAMI, IJCV, TIP, TVCG, TNNLS, TMM, TCSVT, CVIU, etc. <br />
Reviewer for Conferences: CVPR, ICCV, ECCV, SIGGRAPH, NeurIPS, ICLR, ICML, 3DV, etc.

<span class='anchor' id='-collaborators'></span>

# ğŸŒ Collaborators

I have been fortunately advised by and working with:<br />
<a href="https://liuyebin.com" target="_blank">Prof. Yebin Liu åˆ˜çƒ¨æ–Œ (Tsinghua University)</a>,<br />
<a href="http://www.cbsr.ia.ac.cn/users/znsun" target="_blank">Prof. Zhenan Sun å­™å“²å— (Institute of Automation, CAS)</a>,<br />
<a href="https://wlouyang.github.io" target="_blank">Prof. Wanli Ouyang æ¬§é˜³ä¸‡é‡Œ (Shanghai AI Lab)</a>,<br />
and <a href="https://scholar.google.com/citations?hl=en&user=6z0m_ZMAAAAJ#d=gsc_md_cod&t=1714397341598&u=%2Fcitations%3Fview_op%3Dlist_colleagues%26hl%3Den%26json%3D%26user%3D6z0m_ZMAAAAJ%23t%3Dgsc_cod_lc" target="_blank">More</a>.


<!-- PostDoc, <strong>Tsinghua University</strong> (Advisor: <a href="https://liuyebin.com" target="_blank">Prof. Yebin Liu</a>)<br /> 
Ph.D., <strong>Institute of Automation, Chinese Academy of Sciences</strong> (Advisor: <a href="http://www.cbsr.ia.ac.cn/users/znsun" target="_blank">Prof. Zhenan Sun</a>)<br /> 
Visiting Student, <strong>The University of Sydney</strong> (Advisor: <a href="https://wlouyang.github.io" target="_blank">Prof. Wanli Ouyang</a>)<br /> 
B.E., <strong>South China University of Technology</strong> -->



<br />

<br />

<br />


<!-- <a ><img style="height:0;" src='//clustrmaps.com/map_v2.png?cl=080808&w=109&t=n&d=YixTrW_BMdr5L3rb__AgAOkCfxEXKHagTEWHnPVvoAI&co=ffffff&ct=808080'></a> -->

